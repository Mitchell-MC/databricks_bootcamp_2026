{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer EDA - Local Version\n",
    "\n",
    "This notebook performs exploratory analysis of GitHub Archive data locally using pandas.\n",
    "Use this for development and testing before deploying to Databricks.\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install pandas requests matplotlib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_DATE = date(2024, 1, 15)  # Adjust to a recent date with available data\n",
    "SAMPLE_HOUR = 10  # Single hour for quick analysis\n",
    "\n",
    "print(f\"Sample date: {SAMPLE_DATE}\")\n",
    "print(f\"Sample hour: {SAMPLE_HOUR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_gharchive_hour(archive_date: date, hour: int) -> list[dict]:\n",
    "    \"\"\"Download and parse one hour of GH Archive data.\"\"\"\n",
    "    url = f\"https://data.gharchive.org/{archive_date.isoformat()}-{hour}.json.gz\"\n",
    "    print(f\"Downloading: {url}\")\n",
    "    \n",
    "    response = requests.get(url, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    decompressed = gzip.decompress(response.content).decode('utf-8')\n",
    "    events = []\n",
    "    \n",
    "    for line in decompressed.strip().split('\\n'):\n",
    "        if line:\n",
    "            try:\n",
    "                events.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(f\"Parsed {len(events):,} events\")\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data\n",
    "events = download_gharchive_hour(SAMPLE_DATE, SAMPLE_HOUR)\n",
    "print(f\"\\nTotal events: {len(events):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key fields into a DataFrame\n",
    "records = []\n",
    "for event in events:\n",
    "    records.append({\n",
    "        'event_id': event.get('id'),\n",
    "        'event_type': event.get('type'),\n",
    "        'actor_id': event.get('actor', {}).get('id'),\n",
    "        'actor_login': event.get('actor', {}).get('login'),\n",
    "        'repo_id': event.get('repo', {}).get('id'),\n",
    "        'repo_name': event.get('repo', {}).get('name'),\n",
    "        'created_at': event.get('created_at'),\n",
    "        'public': event.get('public'),\n",
    "        'payload_action': event.get('payload', {}).get('action'),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Event Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event type distribution\n",
    "event_counts = df['event_type'].value_counts()\n",
    "event_pct = df['event_type'].value_counts(normalize=True) * 100\n",
    "\n",
    "event_summary = pd.DataFrame({\n",
    "    'count': event_counts,\n",
    "    'percentage': event_pct.round(2)\n",
    "})\n",
    "\n",
    "print(\"Event Type Distribution:\")\n",
    "event_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize event types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart\n",
    "event_counts.plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Event Type')\n",
    "axes[0].set_title('GitHub Event Type Distribution')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Pie chart (top 5)\n",
    "top_5 = event_counts.head(5)\n",
    "other = event_counts[5:].sum()\n",
    "pie_data = pd.concat([top_5, pd.Series({'Other': other})])\n",
    "pie_data.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_title('Top 5 Event Types')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event categories\n",
    "def categorize_event(event_type):\n",
    "    categories = {\n",
    "        'Code Activity': ['PushEvent', 'CreateEvent', 'DeleteEvent', 'CommitCommentEvent'],\n",
    "        'Pull Requests': ['PullRequestEvent', 'PullRequestReviewEvent', 'PullRequestReviewCommentEvent'],\n",
    "        'Issues': ['IssuesEvent', 'IssueCommentEvent'],\n",
    "        'Social': ['WatchEvent', 'ForkEvent'],\n",
    "        'Releases': ['ReleaseEvent'],\n",
    "    }\n",
    "    for cat, types in categories.items():\n",
    "        if event_type in types:\n",
    "            return cat\n",
    "    return 'Other'\n",
    "\n",
    "df['category'] = df['event_type'].apply(categorize_event)\n",
    "category_counts = df['category'].value_counts()\n",
    "\n",
    "print(\"\\nEvent Categories:\")\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top actors by event count\n",
    "top_actors = df.groupby('actor_login').agg(\n",
    "    event_count=('event_id', 'count'),\n",
    "    event_types=('event_type', 'nunique'),\n",
    "    repos=('repo_name', 'nunique')\n",
    ").sort_values('event_count', ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 Most Active Users:\")\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor activity distribution\n",
    "actor_activity = df.groupby('actor_login').size()\n",
    "\n",
    "print(f\"\\nActor Activity Statistics:\")\n",
    "print(f\"  Total unique actors: {len(actor_activity):,}\")\n",
    "print(f\"  Mean events per actor: {actor_activity.mean():.2f}\")\n",
    "print(f\"  Median events per actor: {actor_activity.median():.0f}\")\n",
    "print(f\"  Max events by one actor: {actor_activity.max():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Repository Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top repositories by activity\n",
    "top_repos = df.groupby('repo_name').agg(\n",
    "    event_count=('event_id', 'count'),\n",
    "    event_types=('event_type', 'nunique'),\n",
    "    unique_actors=('actor_login', 'nunique')\n",
    ").sort_values('event_count', ascending=False).head(20)\n",
    "\n",
    "print(\"Top 20 Most Active Repositories:\")\n",
    "top_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repos receiving stars (WatchEvents)\n",
    "starred_repos = df[df['event_type'] == 'WatchEvent'].groupby('repo_name').size().sort_values(ascending=False).head(20)\n",
    "\n",
    "print(\"\\nMost Starred Repos (in this sample):\")\n",
    "starred_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events by minute\n",
    "df['minute'] = df['created_at'].dt.minute\n",
    "events_by_minute = df.groupby('minute').size()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "events_by_minute.plot(kind='line', marker='o')\n",
    "plt.xlabel('Minute of Hour')\n",
    "plt.ylabel('Event Count')\n",
    "plt.title(f'Events by Minute (Hour {SAMPLE_HOUR})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null analysis\n",
    "null_counts = df.isnull().sum()\n",
    "null_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'null_count': null_counts,\n",
    "    'null_pct': null_pct\n",
    "})\n",
    "\n",
    "print(\"Null Analysis:\")\n",
    "null_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate event IDs\n",
    "duplicates = df['event_id'].duplicated().sum()\n",
    "print(f\"Duplicate event IDs: {duplicates}\")\n",
    "print(f\"Duplicate rate: {duplicates / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Event Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample raw event (PushEvent)\n",
    "push_events = [e for e in events if e.get('type') == 'PushEvent']\n",
    "if push_events:\n",
    "    print(\"Sample PushEvent:\")\n",
    "    print(json.dumps(push_events[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PushEvent commit analysis\n",
    "push_commits = []\n",
    "for e in push_events:\n",
    "    push_commits.append({\n",
    "        'repo': e['repo']['name'],\n",
    "        'actor': e['actor']['login'],\n",
    "        'commits': e['payload'].get('size', 0),\n",
    "        'branch': e['payload'].get('ref', '').replace('refs/heads/', '')\n",
    "    })\n",
    "\n",
    "push_df = pd.DataFrame(push_commits)\n",
    "print(f\"\\nPushEvent Statistics:\")\n",
    "print(f\"  Total pushes: {len(push_df):,}\")\n",
    "print(f\"  Total commits: {push_df['commits'].sum():,}\")\n",
    "print(f\"  Mean commits per push: {push_df['commits'].mean():.2f}\")\n",
    "print(f\"\\nCommits per push distribution:\")\n",
    "push_df['commits'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EDA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData Source: GH Archive {SAMPLE_DATE} Hour {SAMPLE_HOUR}\")\n",
    "print(f\"\\nVolume:\")\n",
    "print(f\"  Total events: {len(df):,}\")\n",
    "print(f\"  Unique event types: {df['event_type'].nunique()}\")\n",
    "print(f\"  Unique actors: {df['actor_login'].nunique():,}\")\n",
    "print(f\"  Unique repos: {df['repo_name'].nunique():,}\")\n",
    "print(f\"\\nTop Event Types:\")\n",
    "for event_type, count in event_counts.head(5).items():\n",
    "    print(f\"  {event_type}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nData Quality:\")\n",
    "print(f\"  Duplicate rate: {duplicates / len(df) * 100:.2f}%\")\n",
    "print(f\"  Null actor_login: {df['actor_login'].isnull().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
